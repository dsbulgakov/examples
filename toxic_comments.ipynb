{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span><ul class=\"toc-item\"><li><span><a href=\"#Загрузка-и-изучение-данных\" data-toc-modified-id=\"Загрузка-и-изучение-данных-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Загрузка и изучение данных</a></span></li><li><span><a href=\"#Предобработка-данных\" data-toc-modified-id=\"Предобработка-данных-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Предобработка данных</a></span></li></ul></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span><ul class=\"toc-item\"><li><span><a href=\"#Логистическая-регрессия\" data-toc-modified-id=\"Логистическая-регрессия-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Логистическая регрессия</a></span></li><li><span><a href=\"#Константные-модели\" data-toc-modified-id=\"Константные-модели-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Константные модели</a></span></li><li><span><a href=\"#Решающее-дерево\" data-toc-modified-id=\"Решающее-дерево-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Решающее дерево</a></span></li><li><span><a href=\"#Случайный-лес\" data-toc-modified-id=\"Случайный-лес-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Случайный лес</a></span></li><li><span><a href=\"#LightGBM\" data-toc-modified-id=\"LightGBM-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>LightGBM</a></span></li></ul></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install spacy\n",
    "#!{sys.executable} -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, recall_score, precision_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка и изучение данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим данные с сервера, так они будут запускаться в любой среде:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='https://code.s3.yandex.net/datasets/'\n",
    "file_name='toxic_comments.csv'\n",
    "\n",
    "df = pd.read_csv(path+file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изменим максимальную длину столбца и максимальное отображаемое количество строк чтобы почитать комментарии:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 255)\n",
    "pd.options.display.max_rows=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изучим загруженный датасет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired no...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember what page that's on?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the tools well.  · talk \"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Your vandalism to the Matt Shirvington article has been reverted.  Please don't do it again, or you will be banned.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to you. Anyway, I'm not intending to write anything in the article(wow they would jump on me for vandalism), I'm merely requesting that it be more encyclopedic so one can use it for school as a reference. I h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>alignment on this subject and which are contrary to those of DuLithgow</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                             text  \\\n",
       "0  Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired no...   \n",
       "1                                                                                                                                                D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)   \n",
       "2                       Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie d...   \n",
       "4                                                                                                                                                                                             You, sir, are my hero. Any chance you remember what page that's on?   \n",
       "5                                                                                                                                                                                             \"\\n\\nCongratulations from me as well, use the tools well.  · talk \"   \n",
       "6                                                                                                                                                                                                                    COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK   \n",
       "7                                                                                                                                             Your vandalism to the Matt Shirvington article has been reverted.  Please don't do it again, or you will be banned.   \n",
       "8  Sorry if the word 'nonsense' was offensive to you. Anyway, I'm not intending to write anything in the article(wow they would jump on me for vandalism), I'm merely requesting that it be more encyclopedic so one can use it for school as a reference. I h...   \n",
       "9                                                                                                                                                                                          alignment on this subject and which are contrary to those of DuLithgow   \n",
       "\n",
       "   toxic  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  \n",
       "5      0  \n",
       "6      1  \n",
       "7      0  \n",
       "8      0  \n",
       "9      0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159571 non-null  object\n",
      " 1   toxic   159571 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     0\n",
       "toxic    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем количество токсичных комментариев и их долю:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    143346\n",
       "1     16225\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['toxic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['toxic'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.31876717871444"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['toxic'].value_counts()[1] / df['toxic'].value_counts()[0]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ознакомимся с токсичными комментариями:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Hey... what is it..\\n@ | talk .\\nWhat is it... an exclusive group of some WP TALIBANS...who are good at destroying, self-appointed purist who GANG UP any one who asks them questions abt their ANTI-SOCIAL and DESTRUCTIVE (non)-contribution at WP?\\n\\nAs...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Bye! \\n\\nDon't look, come or think of comming back! Tosser.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>You are gay or antisemmitian? \\n\\nArchangel WHite Tiger\\n\\nMeow! Greetingshhh!\\n\\nUh, there are two ways, why you do erased my comment about WW2, that holocaust was brutally slaying of Jews and not gays/Gypsys/Slavs/anyone...\\n\\n1 - If you are anti-se...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>I'm Sorry \\n\\nI'm sorry I screwed around with someones talk page.  It was very bad to do.  I know how having the templates on their talk page helps you assert your dominance over them.  I know I should bow down to the almighty administrators.  But the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>GET FUCKED UP. GET FUCKEEED UP.  GOT A DRINK THAT YOU CANT PUT DOWN???/ GET FUCK UP GET FUCKED UP.  I'M FUCKED UP RIGHT NOW!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Stupid peace of shit stop deleting my stuff asshole go die and fall in a hole go to hell!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>=Tony Sidaway is obviously a fistfuckee. He loves an arm up his ass.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>My Band Page's deletion. You thought I was gone. \\n\\nDeleting the comment I posted on your 'talk page' does not delete my feelings for your hasty decision to delete my page. You, sir, are still a cock-sucking douche fuck. Sit back for a second and thi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                              text  \\\n",
       "6                                                                                                                                                                                                                     COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK   \n",
       "12  Hey... what is it..\\n@ | talk .\\nWhat is it... an exclusive group of some WP TALIBANS...who are good at destroying, self-appointed purist who GANG UP any one who asks them questions abt their ANTI-SOCIAL and DESTRUCTIVE (non)-contribution at WP?\\n\\nAs...   \n",
       "16                                                                                                                                                                                                     Bye! \\n\\nDon't look, come or think of comming back! Tosser.   \n",
       "42  You are gay or antisemmitian? \\n\\nArchangel WHite Tiger\\n\\nMeow! Greetingshhh!\\n\\nUh, there are two ways, why you do erased my comment about WW2, that holocaust was brutally slaying of Jews and not gays/Gypsys/Slavs/anyone...\\n\\n1 - If you are anti-se...   \n",
       "43                                                                                                                                                                                                                        FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!   \n",
       "44  I'm Sorry \\n\\nI'm sorry I screwed around with someones talk page.  It was very bad to do.  I know how having the templates on their talk page helps you assert your dominance over them.  I know I should bow down to the almighty administrators.  But the...   \n",
       "51                                                                                                                                    GET FUCKED UP. GET FUCKEEED UP.  GOT A DRINK THAT YOU CANT PUT DOWN???/ GET FUCK UP GET FUCKED UP.  I'M FUCKED UP RIGHT NOW!   \n",
       "55                                                                                                                                                                       Stupid peace of shit stop deleting my stuff asshole go die and fall in a hole go to hell!   \n",
       "56                                                                                                                                                                                            =Tony Sidaway is obviously a fistfuckee. He loves an arm up his ass.   \n",
       "58  My Band Page's deletion. You thought I was gone. \\n\\nDeleting the comment I posted on your 'talk page' does not delete my feelings for your hasty decision to delete my page. You, sir, are still a cock-sucking douche fuck. Sit back for a second and thi...   \n",
       "\n",
       "    toxic  \n",
       "6       1  \n",
       "12      1  \n",
       "16      1  \n",
       "42      1  \n",
       "43      1  \n",
       "44      1  \n",
       "51      1  \n",
       "55      1  \n",
       "56      1  \n",
       "58      1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query('toxic==1').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Датасет содержит 159571 строку, каждая из которых - комментарий пользователя. \n",
    "* Есть 2 признака - сам текст и его токсичность (да/нет).\n",
    "* Пропусков и дубликатов в данных не обнаружено\n",
    "* Типы данных корректные, но в целях экономии памяти можно токсичность преобразовать в булев тип, он содержит только 1 или 0.\n",
    "* Доля токсичных комментариев 11.32%, это означет, что при обучении моделей предстоит перебороть существенный дисбаланс классов\n",
    "* Комментарии с toxic=1 действительно токсичные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предобработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изменим тип ключевого признака на булев:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['toxic'] = df['toxic'].astype('bool')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизируем и лемматизируем англоязычный текст:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция со Stack Overflow для токенизации+лемматизации текста\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "df['lemm_text'] = df.text.apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>lemm_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired no...</td>\n",
       "      <td>False</td>\n",
       "      <td>[Explanation, Why, the, edits, made, under, my, username, Hardcore, Metallica, Fan, were, reverted?, They, weren't, vandalisms,, just, closure, on, some, GAs, after, I, voted, at, New, York, Dolls, FAC., And, please, don't, remove, the, template, from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)</td>\n",
       "      <td>False</td>\n",
       "      <td>[D'aww!, He, match, this, background, colour, I'm, seemingly, stuck, with., Thanks., (talk), 21:51,, January, 11,, 2016, (UTC)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.</td>\n",
       "      <td>False</td>\n",
       "      <td>[Hey, man,, I'm, really, not, trying, to, edit, war., It's, just, that, this, guy, is, constantly, removing, relevant, information, and, talking, to, me, through, edits, instead, of, my, talk, page., He, seems, to, care, more, about, the, formatting, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie d...</td>\n",
       "      <td>False</td>\n",
       "      <td>[\", More, I, can't, make, any, real, suggestion, on, improvement, -, I, wondered, if, the, section, statistic, should, be, later, on,, or, a, subsection, of, \"\"types, of, accidents\"\", -I, think, the, reference, may, need, tidying, so, that, they, are,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember what page that's on?</td>\n",
       "      <td>False</td>\n",
       "      <td>[You,, sir,, are, my, hero., Any, chance, you, remember, what, page, that's, on?]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                             text  \\\n",
       "0  Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired no...   \n",
       "1                                                                                                                                                D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)   \n",
       "2                       Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie d...   \n",
       "4                                                                                                                                                                                             You, sir, are my hero. Any chance you remember what page that's on?   \n",
       "\n",
       "   toxic  \\\n",
       "0  False   \n",
       "1  False   \n",
       "2  False   \n",
       "3  False   \n",
       "4  False   \n",
       "\n",
       "                                                                                                                                                                                                                                                        lemm_text  \n",
       "0  [Explanation, Why, the, edits, made, under, my, username, Hardcore, Metallica, Fan, were, reverted?, They, weren't, vandalisms,, just, closure, on, some, GAs, after, I, voted, at, New, York, Dolls, FAC., And, please, don't, remove, the, template, from...  \n",
       "1                                                                                                                                 [D'aww!, He, match, this, background, colour, I'm, seemingly, stuck, with., Thanks., (talk), 21:51,, January, 11,, 2016, (UTC)]  \n",
       "2  [Hey, man,, I'm, really, not, trying, to, edit, war., It's, just, that, this, guy, is, constantly, removing, relevant, information, and, talking, to, me, through, edits, instead, of, my, talk, page., He, seems, to, care, more, about, the, formatting, ...  \n",
       "3  [\", More, I, can't, make, any, real, suggestion, on, improvement, -, I, wondered, if, the, section, statistic, should, be, later, on,, or, a, subsection, of, \"\"types, of, accidents\"\", -I, think, the, reference, may, need, tidying, so, that, they, are,...  \n",
       "4                                                                                                                                                                               [You,, sir,, are, my, hero., Any, chance, you, remember, what, page, that's, on?]  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На самом деле довольно слабая лемматизация, например он изменил matches на match, но оставил много других слов не в исходной форме. Чтобы лемматизация в nltk работа адекватно, нужно задавать форму речи. Очевидно, что задать форму речи для такого объема данных практически невозможно и я не вижу вариантов как это можно быстро автоматизировать. Но, к счастью, кто-то уже это сделал поэтому импортируем библиотеку, которая автоматически определяет форму речи и находит правильную лемму для каждого слова, т.е. по сути делает адекватную лемматизацию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим spacy ко всему объему данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56min 7s, sys: 11.1 s, total: 56min 18s\n",
      "Wall time: 57min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['strong_lemm'] = df['text'].apply(lambda x: \" \".join([w.lemma_ for w in nlp(x)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>lemm_text</th>\n",
       "      <th>strong_lemm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired no...</td>\n",
       "      <td>False</td>\n",
       "      <td>[Explanation, Why, the, edits, made, under, my, username, Hardcore, Metallica, Fan, were, reverted?, They, weren't, vandalisms,, just, closure, on, some, GAs, after, I, voted, at, New, York, Dolls, FAC., And, please, don't, remove, the, template, from...</td>\n",
       "      <td>explanation \\n why the edit make under -PRON- username Hardcore Metallica Fan be revert ? -PRON- be not vandalism , just closure on some gas after -PRON- vote at New York Dolls FAC . and please do not remove the template from the talk page since -PRON...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)</td>\n",
       "      <td>False</td>\n",
       "      <td>[D'aww!, He, match, this, background, colour, I'm, seemingly, stuck, with., Thanks., (talk), 21:51,, January, 11,, 2016, (UTC)]</td>\n",
       "      <td>d'aww ! -PRON- match this background colour -PRON- be seemingly stick with . thank .   ( talk ) 21:51 , January 11 , 2016 ( UTC )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.</td>\n",
       "      <td>False</td>\n",
       "      <td>[Hey, man,, I'm, really, not, trying, to, edit, war., It's, just, that, this, guy, is, constantly, removing, relevant, information, and, talking, to, me, through, edits, instead, of, my, talk, page., He, seems, to, care, more, about, the, formatting, ...</td>\n",
       "      <td>hey man , -PRON- be really not try to edit war . -PRON- be just that this guy be constantly remove relevant information and talk to -PRON- through edit instead of -PRON- talk page . -PRON- seem to care more about the formatting than the actual info .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie d...</td>\n",
       "      <td>False</td>\n",
       "      <td>[\", More, I, can't, make, any, real, suggestion, on, improvement, -, I, wondered, if, the, section, statistic, should, be, later, on,, or, a, subsection, of, \"\"types, of, accidents\"\", -I, think, the, reference, may, need, tidying, so, that, they, are,...</td>\n",
       "      <td>\" \\n More \\n -PRON- can not make any real suggestion on improvement - -PRON- wonder if the section statistic should be later on , or a subsection of \" \" type of accident \" \"   -I think the reference may need tidying so that -PRON- be all in the exact ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember what page that's on?</td>\n",
       "      <td>False</td>\n",
       "      <td>[You,, sir,, are, my, hero., Any, chance, you, remember, what, page, that's, on?]</td>\n",
       "      <td>-PRON- , sir , be -PRON- hero . any chance -PRON- remember what page that be on ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                             text  \\\n",
       "0  Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired no...   \n",
       "1                                                                                                                                                D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)   \n",
       "2                       Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie d...   \n",
       "4                                                                                                                                                                                             You, sir, are my hero. Any chance you remember what page that's on?   \n",
       "\n",
       "   toxic  \\\n",
       "0  False   \n",
       "1  False   \n",
       "2  False   \n",
       "3  False   \n",
       "4  False   \n",
       "\n",
       "                                                                                                                                                                                                                                                        lemm_text  \\\n",
       "0  [Explanation, Why, the, edits, made, under, my, username, Hardcore, Metallica, Fan, were, reverted?, They, weren't, vandalisms,, just, closure, on, some, GAs, after, I, voted, at, New, York, Dolls, FAC., And, please, don't, remove, the, template, from...   \n",
       "1                                                                                                                                 [D'aww!, He, match, this, background, colour, I'm, seemingly, stuck, with., Thanks., (talk), 21:51,, January, 11,, 2016, (UTC)]   \n",
       "2  [Hey, man,, I'm, really, not, trying, to, edit, war., It's, just, that, this, guy, is, constantly, removing, relevant, information, and, talking, to, me, through, edits, instead, of, my, talk, page., He, seems, to, care, more, about, the, formatting, ...   \n",
       "3  [\", More, I, can't, make, any, real, suggestion, on, improvement, -, I, wondered, if, the, section, statistic, should, be, later, on,, or, a, subsection, of, \"\"types, of, accidents\"\", -I, think, the, reference, may, need, tidying, so, that, they, are,...   \n",
       "4                                                                                                                                                                               [You,, sir,, are, my, hero., Any, chance, you, remember, what, page, that's, on?]   \n",
       "\n",
       "                                                                                                                                                                                                                                                      strong_lemm  \n",
       "0  explanation \\n why the edit make under -PRON- username Hardcore Metallica Fan be revert ? -PRON- be not vandalism , just closure on some gas after -PRON- vote at New York Dolls FAC . and please do not remove the template from the talk page since -PRON...  \n",
       "1                                                                                                                               d'aww ! -PRON- match this background colour -PRON- be seemingly stick with . thank .   ( talk ) 21:51 , January 11 , 2016 ( UTC )  \n",
       "2      hey man , -PRON- be really not try to edit war . -PRON- be just that this guy be constantly remove relevant information and talk to -PRON- through edit instead of -PRON- talk page . -PRON- seem to care more about the formatting than the actual info .  \n",
       "3  \" \\n More \\n -PRON- can not make any real suggestion on improvement - -PRON- wonder if the section statistic should be later on , or a subsection of \" \" type of accident \" \"   -I think the reference may need tidying so that -PRON- be all in the exact ...  \n",
       "4                                                                                                                                                                               -PRON- , sir , be -PRON- hero . any chance -PRON- remember what page that be on ?  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так намного лучше, хотя и есть вопросы к \"I ca n't\", надеюсь так и должно быть. Но spacy работает с исходным текстом, а nltk с токенизированным, поэтому, на всякий случай, я бы прогнал ещё результат через функцию токенизации+лемматизации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 15s, sys: 1.34 s, total: 1min 17s\n",
      "Wall time: 1min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['lemm_text'] = df.strong_lemm.apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>lemm_text</th>\n",
       "      <th>strong_lemm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired no...</td>\n",
       "      <td>False</td>\n",
       "      <td>[explanation, why, the, edit, make, under, -PRON-, username, Hardcore, Metallica, Fan, be, revert, ?, -PRON-, be, not, vandalism, ,, just, closure, on, some, gas, after, -PRON-, vote, at, New, York, Dolls, FAC, ., and, please, do, not, remove, the, te...</td>\n",
       "      <td>explanation \\n why the edit make under -PRON- username Hardcore Metallica Fan be revert ? -PRON- be not vandalism , just closure on some gas after -PRON- vote at New York Dolls FAC . and please do not remove the template from the talk page since -PRON...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)</td>\n",
       "      <td>False</td>\n",
       "      <td>[d'aww, !, -PRON-, match, this, background, colour, -PRON-, be, seemingly, stick, with, ., thank, ., (, talk, ), 21:51, ,, January, 11, ,, 2016, (, UTC, )]</td>\n",
       "      <td>d'aww ! -PRON- match this background colour -PRON- be seemingly stick with . thank .   ( talk ) 21:51 , January 11 , 2016 ( UTC )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.</td>\n",
       "      <td>False</td>\n",
       "      <td>[hey, man, ,, -PRON-, be, really, not, try, to, edit, war, ., -PRON-, be, just, that, this, guy, be, constantly, remove, relevant, information, and, talk, to, -PRON-, through, edit, instead, of, -PRON-, talk, page, ., -PRON-, seem, to, care, more, abo...</td>\n",
       "      <td>hey man , -PRON- be really not try to edit war . -PRON- be just that this guy be constantly remove relevant information and talk to -PRON- through edit instead of -PRON- talk page . -PRON- seem to care more about the formatting than the actual info .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie d...</td>\n",
       "      <td>False</td>\n",
       "      <td>[\", More, -PRON-, can, not, make, any, real, suggestion, on, improvement, -, -PRON-, wonder, if, the, section, statistic, should, be, later, on, ,, or, a, subsection, of, \", \", type, of, accident, \", \", -I, think, the, reference, may, need, tidying, s...</td>\n",
       "      <td>\" \\n More \\n -PRON- can not make any real suggestion on improvement - -PRON- wonder if the section statistic should be later on , or a subsection of \" \" type of accident \" \"   -I think the reference may need tidying so that -PRON- be all in the exact ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember what page that's on?</td>\n",
       "      <td>False</td>\n",
       "      <td>[-PRON-, ,, sir, ,, be, -PRON-, hero, ., any, chance, -PRON-, remember, what, page, that, be, on, ?]</td>\n",
       "      <td>-PRON- , sir , be -PRON- hero . any chance -PRON- remember what page that be on ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                             text  \\\n",
       "0  Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired no...   \n",
       "1                                                                                                                                                D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)   \n",
       "2                       Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie d...   \n",
       "4                                                                                                                                                                                             You, sir, are my hero. Any chance you remember what page that's on?   \n",
       "\n",
       "   toxic  \\\n",
       "0  False   \n",
       "1  False   \n",
       "2  False   \n",
       "3  False   \n",
       "4  False   \n",
       "\n",
       "                                                                                                                                                                                                                                                        lemm_text  \\\n",
       "0  [explanation, why, the, edit, make, under, -PRON-, username, Hardcore, Metallica, Fan, be, revert, ?, -PRON-, be, not, vandalism, ,, just, closure, on, some, gas, after, -PRON-, vote, at, New, York, Dolls, FAC, ., and, please, do, not, remove, the, te...   \n",
       "1                                                                                                     [d'aww, !, -PRON-, match, this, background, colour, -PRON-, be, seemingly, stick, with, ., thank, ., (, talk, ), 21:51, ,, January, 11, ,, 2016, (, UTC, )]   \n",
       "2  [hey, man, ,, -PRON-, be, really, not, try, to, edit, war, ., -PRON-, be, just, that, this, guy, be, constantly, remove, relevant, information, and, talk, to, -PRON-, through, edit, instead, of, -PRON-, talk, page, ., -PRON-, seem, to, care, more, abo...   \n",
       "3  [\", More, -PRON-, can, not, make, any, real, suggestion, on, improvement, -, -PRON-, wonder, if, the, section, statistic, should, be, later, on, ,, or, a, subsection, of, \", \", type, of, accident, \", \", -I, think, the, reference, may, need, tidying, s...   \n",
       "4                                                                                                                                                            [-PRON-, ,, sir, ,, be, -PRON-, hero, ., any, chance, -PRON-, remember, what, page, that, be, on, ?]   \n",
       "\n",
       "                                                                                                                                                                                                                                                      strong_lemm  \n",
       "0  explanation \\n why the edit make under -PRON- username Hardcore Metallica Fan be revert ? -PRON- be not vandalism , just closure on some gas after -PRON- vote at New York Dolls FAC . and please do not remove the template from the talk page since -PRON...  \n",
       "1                                                                                                                               d'aww ! -PRON- match this background colour -PRON- be seemingly stick with . thank .   ( talk ) 21:51 , January 11 , 2016 ( UTC )  \n",
       "2      hey man , -PRON- be really not try to edit war . -PRON- be just that this guy be constantly remove relevant information and talk to -PRON- through edit instead of -PRON- talk page . -PRON- seem to care more about the formatting than the actual info .  \n",
       "3  \" \\n More \\n -PRON- can not make any real suggestion on improvement - -PRON- wonder if the section statistic should be later on , or a subsection of \" \" type of accident \" \"   -I think the reference may need tidying so that -PRON- be all in the exact ...  \n",
       "4                                                                                                                                                                               -PRON- , sir , be -PRON- hero . any chance -PRON- remember what page that be on ?  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь в lemm_text все комментарии лемматизированны адекватно, strong_lemm и исходный текст можно удалить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['strong_lemm'] = df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['strong_lemm', 'text'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['lemm_text', 'toxic']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemm_text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[explanation, why, the, edit, make, under, -PRON-, username, Hardcore, Metallica, Fan, be, revert, ?, -PRON-, be, not, vandalism, ,, just, closure, on, some, gas, after, -PRON-, vote, at, New, York, Dolls, FAC, ., and, please, do, not, remove, the, te...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[d'aww, !, -PRON-, match, this, background, colour, -PRON-, be, seemingly, stick, with, ., thank, ., (, talk, ), 21:51, ,, January, 11, ,, 2016, (, UTC, )]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[hey, man, ,, -PRON-, be, really, not, try, to, edit, war, ., -PRON-, be, just, that, this, guy, be, constantly, remove, relevant, information, and, talk, to, -PRON-, through, edit, instead, of, -PRON-, talk, page, ., -PRON-, seem, to, care, more, abo...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[\", More, -PRON-, can, not, make, any, real, suggestion, on, improvement, -, -PRON-, wonder, if, the, section, statistic, should, be, later, on, ,, or, a, subsection, of, \", \", type, of, accident, \", \", -I, think, the, reference, may, need, tidying, s...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-PRON-, ,, sir, ,, be, -PRON-, hero, ., any, chance, -PRON-, remember, what, page, that, be, on, ?]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                        lemm_text  \\\n",
       "0  [explanation, why, the, edit, make, under, -PRON-, username, Hardcore, Metallica, Fan, be, revert, ?, -PRON-, be, not, vandalism, ,, just, closure, on, some, gas, after, -PRON-, vote, at, New, York, Dolls, FAC, ., and, please, do, not, remove, the, te...   \n",
       "1                                                                                                     [d'aww, !, -PRON-, match, this, background, colour, -PRON-, be, seemingly, stick, with, ., thank, ., (, talk, ), 21:51, ,, January, 11, ,, 2016, (, UTC, )]   \n",
       "2  [hey, man, ,, -PRON-, be, really, not, try, to, edit, war, ., -PRON-, be, just, that, this, guy, be, constantly, remove, relevant, information, and, talk, to, -PRON-, through, edit, instead, of, -PRON-, talk, page, ., -PRON-, seem, to, care, more, abo...   \n",
       "3  [\", More, -PRON-, can, not, make, any, real, suggestion, on, improvement, -, -PRON-, wonder, if, the, section, statistic, should, be, later, on, ,, or, a, subsection, of, \", \", type, of, accident, \", \", -I, think, the, reference, may, need, tidying, s...   \n",
       "4                                                                                                                                                            [-PRON-, ,, sir, ,, be, -PRON-, hero, ., any, chance, -PRON-, remember, what, page, that, be, on, ?]   \n",
       "\n",
       "   toxic  \n",
       "0  False  \n",
       "1  False  \n",
       "2  False  \n",
       "3  False  \n",
       "4  False  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее очистим текст от лишних знаков и чисел:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation_1(text):\n",
    "    no_punct = \" \".join([c for c in text if c not in string.punctuation])\n",
    "    return no_punct\n",
    "def remove_punctuation_2(text):\n",
    "    no_punct = \"\".join([c for c in text if c not in string.punctuation])\n",
    "    return no_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.82 s, sys: 4 ms, total: 2.82 s\n",
      "Wall time: 2.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data['lemm_text'] = data['lemm_text'].apply(lambda x: remove_punctuation_1(x)) # очистка всех знаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.lemm_text = data.lemm_text.str.replace('\\d+', '') # удаление всех чисел из текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.1 s, sys: 40 ms, total: 10.2 s\n",
      "Wall time: 10.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# очистка вновь появившихся знаков после удаления чисел (разделители)\n",
    "data['lemm_text'] = data['lemm_text'].apply(lambda x: remove_punctuation_2(x)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем все слова в датафрейме к нижнему регистру потому что этого требует TfidfVectorizer по умолчанию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lemm_text'] = data['lemm_text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemm_text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation why the edit make under pron username hardcore metallica fan be revert pron be not vandalism just closure on some gas after pron vote at new york dolls fac and please do not remove the template from the talk page since pron be retire now</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>daww pron match this background colour pron be seemingly stick with thank talk  january   utc</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man pron be really not try to edit war pron be just that this guy be constantly remove relevant information and talk to pron through edit instead of pron talk page pron seem to care more about the formatting than the actual info</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>more pron can not make any real suggestion on improvement pron wonder if the section statistic should be later on or a subsection of type of accident i think the reference may need tidying so that pron be all in the exact same format ie date format et...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pron sir be pron hero any chance pron remember what page that be on</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                        lemm_text  \\\n",
       "0       explanation why the edit make under pron username hardcore metallica fan be revert pron be not vandalism just closure on some gas after pron vote at new york dolls fac and please do not remove the template from the talk page since pron be retire now   \n",
       "1                                                                                                                                                                   daww pron match this background colour pron be seemingly stick with thank talk  january   utc   \n",
       "2                        hey man pron be really not try to edit war pron be just that this guy be constantly remove relevant information and talk to pron through edit instead of pron talk page pron seem to care more about the formatting than the actual info   \n",
       "3  more pron can not make any real suggestion on improvement pron wonder if the section statistic should be later on or a subsection of type of accident i think the reference may need tidying so that pron be all in the exact same format ie date format et...   \n",
       "4                                                                                                                                                                                             pron sir be pron hero any chance pron remember what page that be on   \n",
       "\n",
       "   toxic  \n",
       "0  False  \n",
       "1  False  \n",
       "2  False  \n",
       "3  False  \n",
       "4  False  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь остались только правильно лемматизированные слова, которые отделены пробелами. Разделим выборку на обучающую и тестовую: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.25, random_state=random_state, stratify=data['toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "В обучающую выборку попало данных, %:  74.99984332992837\n",
      "В тестовую выборку попало данных, %:  25.000156670071625\n",
      "Объектов целевого признака в обучающей выборке, %: 11.319052358407204\n",
      "Объектов целевого признака в тестовой выборке, %: 11.317911655551525\n"
     ]
    }
   ],
   "source": [
    "print(\"В обучающую выборку попало данных, %: \", len(train)/len(data)*100)\n",
    "print(\"В тестовую выборку попало данных, %: \", len(test)/len(data)*100)\n",
    "print(\"Объектов целевого признака в обучающей выборке, %:\", \\\n",
    "      (train['toxic'].value_counts()[1] / train['toxic'].value_counts()[0]*100))\n",
    "print(\"Объектов целевого признака в тестовой выборке, %:\", \\\n",
    "      (test['toxic'].value_counts()[1] / test['toxic'].value_counts()[0]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим список стоп-слов и сохраним в переменной, который далее передадим в векторизатор текста:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся векторизатором TfidfVectorizer, в который передадим стоп-слова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> Оставил дефолтный ngrams несмотря на то что с ngrams=(1,2) результат чуть лучше, так как очень сильно раздувается матрица и значительно увеличивается время обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат обучения и трансформации списка текстов сохраним как признаки для обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = vectorizer.fit_transform(train['lemm_text'].values) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В target_train сохраним целевой признак обучающей выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train = train['toxic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Трансформируем обученным векторайзером тексты тестовой выборки и сохраним в переменной features_test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test = vectorizer.transform(test['lemm_text'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В target_test по аналогии сохраним целевой признак тестовой выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_test = test['toxic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перейдем к обучению самих моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Логистическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим логистическую регрессию на трейне. Для этой модели не будем использовать подбор гиперпараметров по сетке, однако установим по умолчанию solver и балансировку классов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=100, multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=42, solver='sag', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_logistic_regression = LogisticRegression(random_state=random_state, solver='sag', class_weight='balanced') \n",
    "model_logistic_regression.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> sag - единственный solver при котором логистическая регрессия не выдаёт ошибки/предупреждения из-за размерности матрицы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем предсказания на тестовой выборке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_logistic_regression = model_logistic_regression.predict(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем f1 на тесте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7420419461300969"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(target_test, predictions_logistic_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В исходном датасете 11.3% положительных значений целевого признака (т.е. единиц). Посмотрим сколько напредиктила модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15444495890728094"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(predictions_logistic_regression).value_counts()[1] / pd.Series(predictions_logistic_regression).value_counts()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Показатель завышен. Это означает, что вероятнее всего в балансе precision/recall будет перекос в пользу recall - модель реже ошибается тогда, когда истинный ответ \"1\", а модель определяет его как \"0\", то есть не определяет токсичный комментарий тогда, когда он действительно таковым является. На мой взгляд, такая ситуация конкретно в этой бизнес задаче - меньшее из зол, ведь мы отправляем комментарий на модерацию и если модератор решит, что комментарий не токсичный, он просто оставит его опубликованным. Обратная сторона - больше работы у модератора. Как бы то ни было, такая метрика очень близка, но всё ещё не соответствует условию по ТЗ (f1>0.75). Также отмечу, что без балансировки классов доля единиц будет наоборот занижена (~8%) и f1 хуже."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы убедиться в выводах с хорошим показателем данной модели по recall и слабым по precision дополнительно посчитаем и эти метрики:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6529885703578789"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(target_test, predictions_logistic_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8592209072978304"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(target_test, predictions_logistic_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводы верны, причём перекос в пользу recall довольно сильный. Для удобства в дальнейшем напишем функцию для нахождения метрик:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test_score(target_test, predictions):\n",
    "    return display('Лучшая F1 мера на тестовой выборке:', f1_score(target_test, predictions)),\\\n",
    "           display ('Precision:', precision_score(target_test, predictions)),\\\n",
    "           display ('Recall:', recall_score(target_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Константные модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В принципе, в данном проекте не вижу смысла проверять константные модели потому что:\n",
    "* Если модель будет проставлять метки сильного класса (нули), то это будет равносильно её отсутствию\n",
    "* Если модель будет проставлять метки слабого класса (единицы), то это всё равно что модератор проверял бы абсолютно все комментарии, то есть это тоже равносильно её отсутствию\n",
    "* Если модель будет проставлять случайно 1 или 0, то очевидно, что у неё будет плохая метрика как минимум из-за дисбаланса классов\n",
    "* Есть четкое требование по ТЗ (f1>0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но на всякий случай всё таки проверим константные модели и оставим в тетрадке лучшую из них:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dummy = DummyClassifier(strategy=\"uniform\")\n",
    "model_dummy.fit(features_train, target_train)\n",
    "predictions_dummy = model_dummy.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Лучшая F1 мера на тестовой выборке:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.1691795085381091"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Precision:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.10178410343790718"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Recall:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.5007396449704142"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_test_score(target_test, predictions_dummy);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как и ожидалось, рандомная дамми-модель самая лучшая из константных моделей, но F1 у неё при этом очень низкая."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Решающее дерево"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подберем параметры для дерева с использованием RandomSearchCV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_tree = {\n",
    "            'max_depth':[2, 4, 6, 8, 10, 14, 20],\n",
    "            'max_features': ['auto', 'sqrt', 'log2', None],\n",
    "            'min_samples_leaf': [1, 3, 5, 7, 9, 11, 15],\n",
    "            'min_samples_split': [2, 4, 6, 8, 10, 14, 20],\n",
    "            'class_weight': ['balanced', None],\n",
    "              }"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "model_tree = DecisionTreeClassifier(random_state=random_state)\n",
    "rs_cv_tree = RandomizedSearchCV(model_tree, params_tree, random_state=random_state, n_jobs=-1,\n",
    "                                cv=3,\n",
    "                                scoring='f1', \n",
    "                                verbose=1, \n",
    "                                n_iter=50)\n",
    "\n",
    "rs_cv_tree.fit(features_train, target_train)\n",
    "best_random_tree = rs_cv_tree.best_estimator_\n",
    "best_params_tree = rs_cv_tree.best_params_\n",
    "best_score_tree = rs_cv_tree.best_score_\n",
    "print('Лучший F1 скор на кросс-валидации: ', best_score_tree)\n",
    "print('При параметрах: ', best_params_tree)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Лучший F1 скор на кросс-валидации:  0.6501885076409691\n",
    "При параметрах:  {'min_samples_split': 14, 'min_samples_leaf': 3, 'max_features': None, 'max_depth': 20, 'class_weight': None}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интересно, что балансировка классов в данном случае не нужна. Обучим модель решающего дерева с найденными параметрами и сделаем предикты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tree = DecisionTreeClassifier(max_depth=20, min_samples_leaf=3, min_samples_split=14, random_state=42)\n",
    "model_tree.fit(features_train, target_train)\n",
    "predictions_tree = model_tree.predict(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем метрики <b> дерева</b> на тесте:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Лучшая F1 мера на тестовой выборке:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6455755677368834"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Precision:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8849291541434092"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Recall:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.5081360946745562"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_test_score(target_test, predictions_tree);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Дерево показазывает заметно хуже F1, чем логистическая регрессия. При этом наблюдается противоположная ситуация с балансом Precision/Recall - сильный перекос в сторону Precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Случайный лес"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подберем параметры для случайного леса с использованием RandomSearchCV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_forest = {\n",
    "            'n_estimators': [100, 150, 200, 250],\n",
    "            'max_depth':[6, 8, 10, 14, 20, 24],\n",
    "            'max_features': ['auto', 'sqrt', 'log2', None],\n",
    "            'min_samples_leaf': [1, 3, 5, 7, 9, 11, 15],\n",
    "            'min_samples_split': [2, 4, 6, 8, 10, 14, 20],\n",
    "            'class_weight': ['balanced', None],\n",
    "              }"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "model_forest = RandomForestClassifier(random_state=random_state)\n",
    "rs_cv_forest = RandomizedSearchCV(model_forest, params_forest, random_state=random_state, n_jobs=-1,\n",
    "                                cv=3,\n",
    "                                scoring='f1', \n",
    "                                verbose=1, \n",
    "                                n_iter=50)\n",
    "\n",
    "rs_cv_forest.fit(features_train, target_train)\n",
    "best_random_forest = rs_cv_forest.best_estimator_\n",
    "best_params_forest = rs_cv_forest.best_params_\n",
    "best_score_forest = rs_cv_forest.best_score_\n",
    "print('Лучший F1 скор на кросс-валидации: ', best_score_forest)\n",
    "print('При параметрах: ', best_params_forest)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Лучший F1 скор на кросс-валидации:  0.6762623567975025\n",
    "При параметрах:  {'n_estimators': 200, 'min_samples_split': 4, 'min_samples_leaf': 5, 'max_features': None, 'max_depth': 24, 'class_weight': None}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модель случайного леса с найденными параметрами и сделаем предикты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_forest = RandomForestClassifier(max_depth=24, max_features=None, min_samples_leaf=5,\n",
    "                       min_samples_split=4, n_estimators=200, random_state=42, n_jobs=-1)\n",
    "model_forest.fit(features_train, target_train)\n",
    "predictions_forest = model_forest.predict(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдем метрики <b>случайного леса</b> на тестовой выборке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Лучшая F1 мера на тестовой выборке:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6714372716199757"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Precision:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8777866242038217"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Recall:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.5436390532544378"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_test_score(target_test, predictions_forest);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат работы леса как обычно лучше дерева (0.64), но всё ещё уступает логистической регрессии (0.74). Также ожидаемо наблюдается перекос в Precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модель градиентного бустинга. Начнем с LightGBM из-за невероятной скорости его работы. Если удастся достичь нужного результата, то другие модели бустинга проверять не будем. В этом проекте на вход подаётся очень большая матрица и обучение (особенно подбор гиперпараметров) занимает очень много времени."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lgb = {\n",
    "    'num_leaves' : [5, 7, 10, 15, 25],\n",
    "    'learning_rate' : [0.03, 0.1, 0.2, 0.3],\n",
    "    'max_depth' : [4, 8, 12, 16, 20, 24, 30],\n",
    "    'n_estimators': [150, 200, 250, 300, 350],\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "rs_cv_lgb = RandomizedSearchCV(lgb.LGBMClassifier(random_state=random_state), params_lgb, \n",
    "                            scoring='f1', \n",
    "                            cv=3, \n",
    "                            verbose=10,\n",
    "                            n_iter=50, \n",
    "                            n_jobs=-1)\n",
    "\n",
    "rs_cv_lgb.fit(features_train, target_train)\n",
    "\n",
    "best_random_lgb = rs_cv_lgb.best_estimator_\n",
    "best_params_lgb = rs_cv_lgb.best_params_\n",
    "best_score_lgb = rs_cv_lgb.best_score_\n",
    "print('Лучший F1 скор на кросс-валидации: ', best_score_lgb)\n",
    "print('При параметрах: ', best_params_lgb)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Лучший F1 скор на кросс-валидации:  0.7651621736946869\n",
    "При параметрах:  {'num_leaves': 25, 'n_estimators': 350, 'max_depth': 20, 'learning_rate': 0.2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 на кросс-валидации лучше, чем у других моделей, обучим модель с подобранными гиперпараметрами и проверим на тесте:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lgb = lgb.LGBMClassifier(random_state=random_state, n_jobs=-1,\n",
    "                              num_leaves=25,\n",
    "                              n_estimators=350,\n",
    "                              max_depth=20,\n",
    "                              learning_rate=0.2)\n",
    "\n",
    "model_lgb.fit(features_train, target_train)\n",
    "\n",
    "predictions_lgb = model_lgb.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Лучшая F1 мера на тестовой выборке:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.7766990291262136"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Precision:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8719680687749463"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Recall:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.7001972386587771"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_test_score(target_test, predictions_lgb);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 почти 0.78! Но, к сожелнию, в балансе precision/recall перекос в пользу Precision. Это означает, что модель будет реже реагировать на токсичные комментарии, чем хотелось бы. Зато у модератора будет меньше работы. Тем не менее, попробуем откалибровать модель. Самый простой способ сделать это автоматически - установить class_weight=balanced. Тогда модель будет более уверена в ответах слабого класса и нарисует их больше. Не очень хочется заново перебирать гиперпараметры по сетке (хотя по хорошему именно так нужно поступить), поэтому попробуем просто добавить текущей модели уверенности через class_weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lgb_balanced = lgb.LGBMClassifier(random_state=random_state, n_jobs=-1,\n",
    "                              num_leaves=25,\n",
    "                              n_estimators=350,\n",
    "                              max_depth=20,\n",
    "                              learning_rate=0.2,\n",
    "                              class_weight='balanced')\n",
    "\n",
    "model_lgb_balanced.fit(features_train, target_train)\n",
    "\n",
    "predictions_lgb_balanced = model_lgb_balanced.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Лучшая F1 мера на тестовой выборке:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.7580735277684657"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Precision:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6932352340077662"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Recall:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8362919132149902"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_test_score(target_test, predictions_lgb_balanced);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Порог F1 в 0.75 пройден и одновременно удалось достичь ожидаемого результата в балансе классов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В проекте обработан большой массив текстовых данных с разметкой: комментарии пользователей интернет-магазина \"Викишоп\". На его основе обучено несколько моделей, которые помогут находить токсичные комментарии пользоваталей. Метрики моделей сведем в таблицу:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://imgur.com/hf2VSid\"><img src=\"https://i.imgur.com/hf2VSid.png\" title=\"source: imgur.com\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Требуемый уровень качества по ТЗ (f1>0.75) превосходят две модели - LightGBM и LightGBM balanced. Я бы рекомендовал заказчику внедрить вторую модель LightGBM balanced, так как у неё намного выше метрика Recall. Это означает, что по сравнению с первой, эта модель реже будет ошибаться в определении действительно токсичного комментария как нетоксичного, но чаще будет отправлять на модерацию нетоксичный комментарий, ошибочно решив, что он токсичный. Таким образом, с точки зрения улучшения качества контента на сайте (минимизации количества токсичных комментариев) лучше подходит вторая модель. Однако, если стоит задача снизить нагрузку на модераторов, то лучше использовать первую - она будет отправлять на модерацию намного меньше комментарив из-за более высокой точности (Precision), но при этом чаще будет игнорировать токсичные комментарии (примерно в 30% случаев)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё короче и понятнее будет в среднем так:\n",
    "\n",
    "<br>Модель 1 (LightGBM):\n",
    "* Отправляет 8.8% комментариев на модерацию (меньше нагрузка на модераторов), при этом лишь 12.1% из них не являются токсичными\n",
    "* Не замечает ~30% действительно токсичных комментариев на сайте\n",
    "\n",
    "<br>Модель 2 (LightGBM balanced):\n",
    "* Отправляет 13.6% комментариев на модерацию (больше нагрузка на модераторов), при этом 29.7% из них не являются токсичными\n",
    "* Не замечает лишь ~17% действительно токсичных комментариев на сайте\n",
    "\n",
    "<br><b>Модель 1 - про оптимизацию затрат на модерацию\n",
    "<br> Модель 2 - про качество контента на сайте"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
